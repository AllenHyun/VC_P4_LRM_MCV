{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64dbf540",
   "metadata": {},
   "source": [
    "Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc881425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import easyocr\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import Video, display\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728c5aa",
   "metadata": {},
   "source": [
    "Extraemos las clases del modelo YOLO 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23885f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "vid = cv2.VideoCapture(\"C0142.MP4\")\n",
    "\n",
    "names = None\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if ret:\n",
    "        results = model(frame, show=False)\n",
    "        if names is None:\n",
    "            names = results[0].names\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"Deteccion de YOLO\", annotated_frame)\n",
    "\n",
    "        # Salir del vídeo cuando presionamos ESC\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or cv2.getWindowProperty(\"Deteccion de YOLO\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "    else:\n",
    "        # El vídeo ya se terminó\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Leemos las posibles clases\n",
    "with open(\"classes.txt\", \"w\") as f:\n",
    "    f.write(str(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d49077",
   "metadata": {},
   "source": [
    "### Mostramos el funcionamiento de nuestro modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('best.pt')\n",
    "\n",
    "vid = cv2.VideoCapture(\"C0142.MP4\")\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if ret:\n",
    "        results = model(frame, show=False)\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"Deteccion de YOLO\", annotated_frame)\n",
    "\n",
    "        # Salir del vídeo cuando presionamos ESC\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or cv2.getWindowProperty(\"Deteccion de YOLO\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "    else:\n",
    "        # El vídeo ya se terminó\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0a81a",
   "metadata": {},
   "source": [
    "### Usamos el modelo pre-entrenado de YOLO y el nuestro en conjunto \n",
    "Utilizamos el modelo pre-entrenado para detectar personas y vehículos, posteriormente, cuando hayamos detectado un vehículo, se lo pasamos a nuestro modelo entrenado en matrículas para que le detecte la matrícula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d241ad",
   "metadata": {},
   "source": [
    "Código para las detecciones de OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1298f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código necesario para el VLM\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "device = \"cpu\"  # or \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "                                                dtype=torch.bfloat16,\n",
    "                                                _attn_implementation=\"flash_attention_2\" if device == \"cuda\" else \"eager\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a69c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Solo se está usando en el Tesseract (Se elimina?)\n",
    "def preprocess_for_ocr(img_crop, escala=4):\n",
    "\n",
    "    img = cv2.resize(img_crop, None, fx=escala, fy=escala, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gray_blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray_blur,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        13,\n",
    "        2\n",
    "    )\n",
    "\n",
    "    return thresh\n",
    "\n",
    "reader = easyocr.Reader(['es'], gpu=False) \n",
    "\n",
    "def ocr_easy(placa_crop, frame, x1, y1, last_plate=None):\n",
    "    escala = 3\n",
    "    placa_crop = cv2.resize(placa_crop, None, fx=escala, fy=escala, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    gray = cv2.cvtColor(placa_crop, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    gray = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)\n",
    "\n",
    "    ocr_result = reader.readtext(\n",
    "        gray,\n",
    "        allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789',\n",
    "        detail=1\n",
    "    )\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    if len(ocr_result) > 0:\n",
    "        text = ocr_result[0][1].strip()\n",
    "        prob = ocr_result[0][2]\n",
    "\n",
    "        if len(text) >= 4 and prob > 0.5 and text != last_plate:\n",
    "            last_plate = text\n",
    "            timestamp = vid.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "            plate_pattern = re.compile(\"^[0-9]{4}[BCDFGHJKLMNPRSTVWXYZ]{3}$\")\n",
    "            if plate_pattern.match(text.strip()):\n",
    "                print(f\"[{timestamp:.2f}s] Matrícula: {text} (Conf: {prob:.2f})\")\n",
    "                cv2.putText(frame, f'{text}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                return\n",
    "   \n",
    "        return text\n",
    "\n",
    "def ocr_vlm(crop, frame, x1, y1, x2, y2):\n",
    "    plate_img = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Read the text on this license plate.\"}\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=[plate_img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=10)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        plate_text = generated_texts[0].strip()\n",
    "\n",
    "        if \"Assistant: \" in plate_text:\n",
    "            raw_text = plate_text.split(\"Assistant: \")[1]\n",
    "\n",
    "    plate_pattern = re.compile(\"^[0-9]{4}[BCDFGHJKLMNPRSTVWXYZ]{3}$\")\n",
    "    if plate_pattern.match(plate_text.strip()):\n",
    "        cv2.putText(frame, raw_text, (x1, max(30, y1 - 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    return raw_text\n",
    "\n",
    "# NO SE USA (Lo eliminamos cuando esté confirmado que usamos los otros dos)\n",
    "def ocr_tesseract(placa_crop, frame_count, cap, crop_dir=\"crops/\", last_texts=set()):\n",
    "    if placa_crop.size > 0:\n",
    "        gray = preprocess_for_ocr(placa_crop)\n",
    "\n",
    "        # Usando Tesseract\n",
    "        ocr_result = pytesseract.image_to_data(\n",
    "            gray,\n",
    "            output_type=Output.DICT,\n",
    "            config='--psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "        )\n",
    "\n",
    "        n_boxes = len(ocr_result['text'])\n",
    "        for i in range(n_boxes):\n",
    "            text = ocr_result['text'][i].strip().replace(\" \", \"\")\n",
    "            conf = float(ocr_result['conf'][i])\n",
    "            if len(text) >= 7 and conf > 60 and text not in last_texts:\n",
    "                last_texts.add(text)\n",
    "                timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "                print(f\"[{timestamp:.2f}s] Matrícula detectada: {text} (Conf: {conf:.2f})\")\n",
    "\n",
    "                cv2.putText(frame, text, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                crop_filename = os.path.join(crop_dir, f\"{text}_{frame_count}.jpg\")\n",
    "                cv2.imwrite(crop_filename, gray)\n",
    "                print(f\"Guardada imagen: {crop_filename}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e835c4b",
   "metadata": {},
   "source": [
    "Código principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b49dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 10 cars, 74.7ms\n",
      "Speed: 45.1ms preprocess, 74.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 1 matricula, 134.9ms\n",
      "Speed: 4.1ms preprocess, 134.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m real_y2 \u001b[38;5;241m=\u001b[39m py2\u001b[38;5;241m+\u001b[39my1\n\u001b[0;32m     89\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(annotated_frame, (real_x1, real_y1), (real_x2, real_y2), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m plate_text \u001b[38;5;241m=\u001b[39m \u001b[43mocr_vlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_x1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_y1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_x2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_y2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m plate_text \u001b[38;5;241m=\u001b[39m plate_text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plate_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mocr_vlm\u001b[1;34m(crop, frame, x1, y1, x2, y2)\u001b[0m\n\u001b[0;32m     71\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mprompt, images\u001b[38;5;241m=\u001b[39m[plate_img], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 74\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     75\u001b[0m     generated_texts \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     76\u001b[0m     plate_text \u001b[38;5;241m=\u001b[39m generated_texts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\generation\\utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[1;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:901\u001b[0m, in \u001b[0;36mIdefics3ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, labels, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    898\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    902\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    903\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    904\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    905\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    906\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    907\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m    908\u001b[0m     pixel_attention_mask\u001b[38;5;241m=\u001b[39mpixel_attention_mask,\n\u001b[0;32m    909\u001b[0m     image_hidden_states\u001b[38;5;241m=\u001b[39mimage_hidden_states,\n\u001b[0;32m    910\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    911\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    912\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    913\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    914\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    916\u001b[0m )\n\u001b[0;32m    918\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    919\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:721\u001b[0m, in \u001b[0;36mIdefics3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both pixel_values and image_hidden_states at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 721\u001b[0m     image_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    723\u001b[0m     image_hidden_states \u001b[38;5;241m=\u001b[39m image_hidden_states\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:648\u001b[0m, in \u001b[0;36mIdefics3Model.get_image_features\u001b[1;34m(self, pixel_values, pixel_attention_mask)\u001b[0m\n\u001b[0;32m    645\u001b[0m patch_attention_mask \u001b[38;5;241m=\u001b[39m (patches_subgrid\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# Get sequence from the vision encoder\u001b[39;00m\n\u001b[1;32m--> 648\u001b[0m image_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m image_hidden_states\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# Modality projection & resampling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:506\u001b[0m, in \u001b[0;36mIdefics3VisionTransformer.forward\u001b[1;34m(self, pixel_values, patch_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(\u001b[38;5;241m~\u001b[39mpatch_attention_mask):\n\u001b[0;32m    504\u001b[0m     patch_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    512\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_layernorm(last_hidden_state)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:346\u001b[0m, in \u001b[0;36mIdefics3Encoder.forward\u001b[1;34m(self, inputs_embeds, attention_mask)\u001b[0m\n\u001b[0;32m    344\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 346\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:315\u001b[0m, in \u001b[0;36mIdefics3EncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    314\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[1;32m--> 315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\transformers\\models\\idefics3\\modeling_idefics3.py:270\u001b[0m, in \u001b[0;36mIdefics3VisionMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    268\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[0;32m    269\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[1;32m--> 270\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lllrm\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_MODEL_PATH = 'yolo11n.pt'\n",
    "OUR_MODEL_PATH = 'best.pt'\n",
    "\n",
    "VIDEO_PATH = \"Copy of C0142_1.mp4\"\n",
    "base_model = YOLO(BASE_MODEL_PATH)\n",
    "our_model = YOLO(OUR_MODEL_PATH)\n",
    "vid = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "frame_width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "output_path = 'resultados.mp4'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "def data_to_csv(registros):\n",
    "    columnas = [\n",
    "        \"fotograma\", \"tipo_objeto\", \"confianza\", \"id_tracking\",\n",
    "        \"x1\", \"y1\", \"x2\", \"y2\",\n",
    "        \"matricula_detectada\", \"conf_ocr\",\n",
    "        \"mx1\", \"my1\", \"mx2\", \"my2\",\n",
    "        \"texto_matricula\"\n",
    "    ]\n",
    "\n",
    "    with open(\"resultados.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerow(columnas)    \n",
    "        writer.writerows(registros)  \n",
    "\n",
    "    print(\"Archivo 'resultados.csv' creado correctamente.\")\n",
    "\n",
    "\n",
    "classes = [0, 2, 3, 5, 7]    # Person, car, motorcycle, bus, truck\n",
    "\n",
    "car_boxes = []\n",
    "car_boxes_left_coords = []\n",
    "\n",
    "track_ids = set()\n",
    "count_classes = {\"person\": 0, \"car\": 0, \"motorcycle\": 0, \"bus\": 0, \"truck\": 0}\n",
    "\n",
    "save_csv = []\n",
    "frame_count = 0\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    if ret:\n",
    "        base_results = base_model.track(frame, persist=True, show=False, classes=classes)\n",
    "        plates_result = None\n",
    "        annotated_frame = base_results[0].plot()\n",
    "        boxes = list()\n",
    "\n",
    "        # Mostramos un recuadro arriba a la izquierda que muestre las matrículas que se vayan detectando\n",
    "        text_box_w = int(frame.shape[1]*0.25)\n",
    "        text_box_h = int(frame.shape[0]*0.09)\n",
    "        \n",
    "        cv2.rectangle(annotated_frame, (0, 0), (text_box_w, text_box_h), (0, 0, 0), -1)\n",
    "\n",
    "        last_plate = \"\"\n",
    "        show_plate_text = \"\"\n",
    "        \n",
    "        for result in base_results:\n",
    "            boxes += result.boxes\n",
    "        for box in boxes:\n",
    "            bounding_box = box.xyxy.tolist()\n",
    "            name = result[0].names[box.cls.int().item()]\n",
    "            conf = box.conf\n",
    "            track_id = str(int(box.id[0].tolist()))\n",
    "            if track_id not in track_ids:\n",
    "                track_ids.add(track_id)\n",
    "                count_classes[name] += 1\n",
    "            x1, y1, x2, y2 = [int(item) for item in bounding_box[0]]\n",
    "            plate, plate_conf, px1, py1, px2, py2, plate_text = \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "            if name != \"person\":\n",
    "                vehicle_box = frame[y1:y2, x1:x2]\n",
    "                plates_result = our_model(vehicle_box, show=False)\n",
    "                if len(plates_result[0].boxes) > 0:\n",
    "                    plate_conf = plates_result[0].boxes.conf\n",
    "                    plate_detection = (plates_result[0].boxes.xyxy).tolist()\n",
    "                    px1, py1, px2, py2 = [int(item) for item in plate_detection[0]]\n",
    "                    plate = vehicle_box[py1:py2, px1:px2]\n",
    "                    real_x1 = px1+x1\n",
    "                    real_y1 = py1+y1\n",
    "                    real_x2 = px2+x1\n",
    "                    real_y2 = py2+y1\n",
    "                    cv2.rectangle(annotated_frame, (real_x1, real_y1), (real_x2, real_y2), (0, 255, 0), 2)\n",
    "                    plate_text = ocr_easy(plate, frame, real_x1, real_y1)\n",
    "                    plate_text = plate_text.strip()\n",
    "                    if plate_text is not None:\n",
    "                        show_plate_text = plate_text\n",
    "            save_csv.append([\"frame\", name, conf, track_id, x1, y1, x2, y2, \"plate\", plate_conf, px1, py1, px2, py2, plate_text])\n",
    "            if show_plate_text != last_plate:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                (text_width, text_height), baseline = cv2.getTextSize(f\"Matrícula detectada: {show_plate_text}\", font, 0.8, 2)\n",
    "                text_x = (text_box_w - text_width) // 2\n",
    "                text_y = (text_box_h + text_height) // 2 - baseline\n",
    "                cv2.putText(annotated_frame, show_plate_text, (text_x, text_y), font, 0.8, (255, 255, 255), 2)\n",
    "                last_plate = show_plate_text\n",
    "            \n",
    "        out.write(annotated_frame)\n",
    "        \"\"\"cv2.imshow(\"Deteccion de YOLO\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == 27 or cv2.getWindowProperty(\"Deteccion de YOLO\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\"\"\"\n",
    "    else:\n",
    "        # El vídeo ya se terminó\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca6f4",
   "metadata": {},
   "source": [
    "### Pruebas OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b308e6",
   "metadata": {},
   "source": [
    "#### Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "\n",
    "model_path = \"best.pt\"\n",
    "video_path = \"C0142.MP4\"\n",
    "crop_dir   = \"crops/\"  \n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "\n",
    "model = YOLO(model_path)\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise Exception(f\"No se pudo abrir el vídeo: {video_path}\")\n",
    "\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_in = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('resultado.mp4', fourcc, fps_in, (width, height))\n",
    "\n",
    "margin = 10\n",
    "frame_count = 0\n",
    "last_texts = set()\n",
    "\n",
    "def preprocess_for_ocr(img_crop, escala=4):\n",
    "\n",
    "    img = cv2.resize(img_crop, None, fx=escala, fy=escala, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gray_blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray_blur,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        13,\n",
    "        2\n",
    "    )\n",
    "\n",
    "    return thresh\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    results = model(frame, verbose=False)\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    for box in detections:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        conf = float(box.conf[0])\n",
    "        if conf < 0.2:\n",
    "            continue\n",
    "\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        extra = int(max(w, h) * 0.15)\n",
    "        x1m = max(0, x1 - margin - extra)\n",
    "        y1m = max(0, y1 - margin - extra)\n",
    "        x2m = min(frame.shape[1], x2 + margin + extra)\n",
    "        y2m = min(frame.shape[0], y2 + margin + extra)\n",
    "\n",
    "        placa_crop = frame[y1m:y2m, x1m:x2m]\n",
    "\n",
    "        if placa_crop.size > 0:\n",
    "\n",
    "            gray = preprocess_for_ocr(placa_crop)\n",
    "\n",
    "            ocr_result = reader.readtext(\n",
    "                gray,\n",
    "                allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789',\n",
    "                detail=1,\n",
    "                text_threshold=0.2\n",
    "            )\n",
    "\n",
    "            if len(ocr_result) > 0:\n",
    "                for (bbox, text, prob) in ocr_result:\n",
    "                    text_clean = text.strip().replace(\" \", \"\")\n",
    "                    if prob > 0.7 and len(text_clean) >= 7 and text_clean not in last_texts:\n",
    "                        last_texts.add(text_clean)\n",
    "                        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "\n",
    "                        print(f\"[{timestamp:.2f}s] Matrícula detectada: {text_clean} (Conf: {prob:.2f})\")\n",
    "\n",
    "                        cv2.putText(frame, text_clean, (x1, y1 - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                        crop_filename = os.path.join(crop_dir, f\"{text_clean}_{frame_count}.jpg\")\n",
    "                        cv2.imwrite(crop_filename, gray)\n",
    "                        print(f\"Guardada imagen: {crop_filename}\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Procesamiento completado. Total frames: {frame_count}\")\n",
    "display(Video('resultado.mp4', embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c874d4",
   "metadata": {},
   "source": [
    "#### Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "# Configura la ruta si Tesseract no está en PATH\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract'\n",
    "\n",
    "model_path = \"best.pt\"\n",
    "video_path = \"C0142.MP4\"\n",
    "crop_dir   = \"crops/\"  \n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise Exception(f\"No se pudo abrir el vídeo: {video_path}\")\n",
    "\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_in = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('resultado_tesseract.mp4', fourcc, fps_in, (width, height))\n",
    "\n",
    "margin = 10\n",
    "frame_count = 0\n",
    "last_texts = set()\n",
    "\n",
    "def preprocess_for_ocr(img_crop, escala=4):\n",
    "    # Escala y convierte a gris\n",
    "    img = cv2.resize(img_crop, None, fx=escala, fy=escala, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    # Umbral adaptativo\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray_blur,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        13,\n",
    "        2\n",
    "    )\n",
    "    return thresh\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "\n",
    "    results = model(frame, verbose=False)\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    for box in detections:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        conf = float(box.conf[0])\n",
    "        if conf < 0.2:\n",
    "            continue\n",
    "\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        extra = int(max(w, h) * 0.15)\n",
    "        x1m = max(0, x1 - margin - extra)\n",
    "        y1m = max(0, y1 - margin - extra)\n",
    "        x2m = min(frame.shape[1], x2 + margin + extra)\n",
    "        y2m = min(frame.shape[0], y2 + margin + extra)\n",
    "\n",
    "        placa_crop = frame[y1m:y2m, x1m:x2m]\n",
    "\n",
    "        if placa_crop.size > 0:\n",
    "            gray = preprocess_for_ocr(placa_crop)\n",
    "\n",
    "            # Usando Tesseract\n",
    "            ocr_result = pytesseract.image_to_data(\n",
    "                gray,\n",
    "                output_type=Output.DICT,\n",
    "                config='--psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "            )\n",
    "\n",
    "            n_boxes = len(ocr_result['text'])\n",
    "            for i in range(n_boxes):\n",
    "                text = ocr_result['text'][i].strip().replace(\" \", \"\")\n",
    "                conf = float(ocr_result['conf'][i])\n",
    "                if len(text) >= 7 and conf > 60 and text not in last_texts:\n",
    "                    last_texts.add(text)\n",
    "                    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "                    print(f\"[{timestamp:.2f}s] Matrícula detectada: {text} (Conf: {conf:.2f})\")\n",
    "\n",
    "                    cv2.putText(frame, text, (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                    crop_filename = os.path.join(crop_dir, f\"{text}_{frame_count}.jpg\")\n",
    "                    cv2.imwrite(crop_filename, gray)\n",
    "                    print(f\"Guardada imagen: {crop_filename}\")\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Procesamiento completado. Total frames: {frame_count}\")\n",
    "display(Video('resultado_tesseract.mp4', embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11dceb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'resultados.csv' creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "registros = [\n",
    "    [1, \"auto\", 0.92, 3, 120, 200, 360, 480, True, 0.88, 135, 215, 345, 465, \"ABC1234\"],\n",
    "    [1, \"moto\", 0.85, 5, 400, 220, 500, 380, False, 0.00, 0, 0, 0, 0, \"\"],\n",
    "    [2, \"auto\", 0.95, 3, 125, 205, 365, 485, True, 0.90, 140, 220, 350, 470, \"ABC1234\"],\n",
    "    [2, \"camioneta\", 0.88, 7, 600, 250, 900, 550, True, 0.75, 620, 270, 880, 530, \"XYZ9876\"],\n",
    "    [3, \"auto\", 0.93, 3, 130, 210, 370, 490, True, 0.85, 145, 225, 355, 475, \"ABC1234\"],\n",
    "    [3, \"moto\", 0.80, 5, 405, 225, 505, 385, False, 0.00, 0, 0, 0, 0, \"\"]\n",
    "]\n",
    "def data_to_csv(registros):\n",
    "    columnas = [\n",
    "        \"fotograma\", \"tipo_objeto\", \"confianza\", \"id_tracking\",\n",
    "        \"x1\", \"y1\", \"x2\", \"y2\",\n",
    "        \"matricula_detectada\", \"conf_ocr\",\n",
    "        \"mx1\", \"my1\", \"mx2\", \"my2\",\n",
    "        \"texto_matricula\"\n",
    "    ]\n",
    "\n",
    "    with open(\"resultados.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=';')\n",
    "        writer.writerow(columnas)    \n",
    "        writer.writerows(registros)  \n",
    "\n",
    "    print(\"Archivo 'resultados.csv' creado correctamente.\")\n",
    "\n",
    "data_to_csv(registros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb0c89",
   "metadata": {},
   "source": [
    "### Comparativa entre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d59981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 300.0ms\n",
      "Speed: 10.5ms preprocess, 300.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:   2%|▏         | 1/50 [00:22<18:09, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 162.2ms\n",
      "Speed: 6.0ms preprocess, 162.2ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:   4%|▍         | 2/50 [00:40<16:03, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 1 matricula, 340.0ms\n",
      "Speed: 7.1ms preprocess, 340.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:   6%|▌         | 3/50 [00:58<14:56, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x320 1 matricula, 113.5ms\n",
      "Speed: 6.8ms preprocess, 113.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:   8%|▊         | 4/50 [01:14<13:34, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 111.0ms\n",
      "Speed: 48.4ms preprocess, 111.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  10%|█         | 5/50 [01:43<16:25, 21.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 518.2ms\n",
      "Speed: 14.9ms preprocess, 518.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  12%|█▏        | 6/50 [02:07<16:39, 22.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s] Matrícula: 0416MLX (Conf: 0.72)\n",
      "\n",
      "0: 640x480 1 matricula, 91.0ms\n",
      "Speed: 71.0ms preprocess, 91.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  14%|█▍        | 7/50 [02:27<15:26, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 71.1ms\n",
      "Speed: 3.7ms preprocess, 71.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  16%|█▌        | 8/50 [02:44<14:12, 20.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s] Matrícula: 0476HNN (Conf: 0.89)\n",
      "\n",
      "0: 640x480 1 matricula, 132.4ms\n",
      "Speed: 69.0ms preprocess, 132.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  18%|█▊        | 9/50 [03:02<13:20, 19.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 87.7ms\n",
      "Speed: 2.9ms preprocess, 87.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  20%|██        | 10/50 [03:19<12:30, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 80.5ms\n",
      "Speed: 46.0ms preprocess, 80.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  22%|██▏       | 11/50 [03:42<13:02, 20.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 2 matriculas, 131.0ms\n",
      "Speed: 6.7ms preprocess, 131.0ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  24%|██▍       | 12/50 [04:09<14:04, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 487.1ms\n",
      "Speed: 6.4ms preprocess, 487.1ms inference, 15.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  26%|██▌       | 13/50 [04:34<14:06, 22.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 170.0ms\n",
      "Speed: 10.2ms preprocess, 170.0ms inference, 15.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  28%|██▊       | 14/50 [04:55<13:26, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 188.8ms\n",
      "Speed: 2.8ms preprocess, 188.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  30%|███       | 15/50 [05:15<12:38, 21.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 2 matriculas, 148.2ms\n",
      "Speed: 9.1ms preprocess, 148.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  32%|███▏      | 16/50 [05:34<11:55, 21.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 221.4ms\n",
      "Speed: 3.1ms preprocess, 221.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  34%|███▍      | 17/50 [05:56<11:39, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 128.9ms\n",
      "Speed: 6.0ms preprocess, 128.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  36%|███▌      | 18/50 [06:17<11:20, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 114.6ms\n",
      "Speed: 70.0ms preprocess, 114.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  38%|███▊      | 19/50 [06:38<10:50, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 157.1ms\n",
      "Speed: 4.6ms preprocess, 157.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  40%|████      | 20/50 [07:12<12:27, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 309.6ms\n",
      "Speed: 29.9ms preprocess, 309.6ms inference, 14.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  42%|████▏     | 21/50 [07:38<12:17, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 264.3ms\n",
      "Speed: 7.5ms preprocess, 264.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  44%|████▍     | 22/50 [07:59<11:14, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 502.9ms\n",
      "Speed: 12.4ms preprocess, 502.9ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  46%|████▌     | 23/50 [08:30<11:45, 26.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 119.6ms\n",
      "Speed: 53.7ms preprocess, 119.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  48%|████▊     | 24/50 [08:54<10:58, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 168.0ms\n",
      "Speed: 5.8ms preprocess, 168.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  50%|█████     | 25/50 [09:17<10:20, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 219.9ms\n",
      "Speed: 5.0ms preprocess, 219.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  52%|█████▏    | 26/50 [09:40<09:38, 24.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 90.5ms\n",
      "Speed: 3.2ms preprocess, 90.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  54%|█████▍    | 27/50 [09:55<08:12, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 73.0ms\n",
      "Speed: 50.5ms preprocess, 73.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  56%|█████▌    | 28/50 [10:15<07:44, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 289.1ms\n",
      "Speed: 7.9ms preprocess, 289.1ms inference, 16.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  58%|█████▊    | 29/50 [10:43<08:05, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 887.1ms\n",
      "Speed: 30.8ms preprocess, 887.1ms inference, 72.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  60%|██████    | 30/50 [11:09<07:57, 23.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 116.4ms\n",
      "Speed: 8.5ms preprocess, 116.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  62%|██████▏   | 31/50 [11:33<07:36, 24.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 226.1ms\n",
      "Speed: 4.1ms preprocess, 226.1ms inference, 11.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  64%|██████▍   | 32/50 [11:57<07:12, 24.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 104.1ms\n",
      "Speed: 5.5ms preprocess, 104.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  66%|██████▌   | 33/50 [12:17<06:28, 22.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 128.2ms\n",
      "Speed: 67.5ms preprocess, 128.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  68%|██████▊   | 34/50 [12:37<05:49, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 138.5ms\n",
      "Speed: 7.5ms preprocess, 138.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  70%|███████   | 35/50 [12:58<05:25, 21.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s] Matrícula: 2753LBJ (Conf: 0.78)\n",
      "\n",
      "0: 640x480 1 matricula, 209.1ms\n",
      "Speed: 60.0ms preprocess, 209.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  72%|███████▏  | 36/50 [13:22<05:13, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 114.1ms\n",
      "Speed: 6.9ms preprocess, 114.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  74%|███████▍  | 37/50 [13:43<04:43, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 185.0ms\n",
      "Speed: 4.2ms preprocess, 185.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  76%|███████▌  | 38/50 [14:05<04:25, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 2 matriculas, 111.7ms\n",
      "Speed: 5.0ms preprocess, 111.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  78%|███████▊  | 39/50 [14:56<05:38, 30.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 2 matriculas, 1076.5ms\n",
      "Speed: 73.6ms preprocess, 1076.5ms inference, 25.4ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  80%|████████  | 40/50 [15:17<04:38, 27.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 68.3ms\n",
      "Speed: 2.6ms preprocess, 68.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  82%|████████▏ | 41/50 [15:33<03:36, 24.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 80.2ms\n",
      "Speed: 62.2ms preprocess, 80.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  84%|████████▍ | 42/50 [15:50<02:55, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 262.3ms\n",
      "Speed: 4.9ms preprocess, 262.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  86%|████████▌ | 43/50 [16:15<02:40, 22.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 matricula, 232.0ms\n",
      "Speed: 5.1ms preprocess, 232.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  88%|████████▊ | 44/50 [16:34<02:11, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 178.6ms\n",
      "Speed: 3.9ms preprocess, 178.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  90%|█████████ | 45/50 [16:58<01:51, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s] Matrícula: 3654MYH (Conf: 0.65)\n",
      "\n",
      "0: 480x640 1 matricula, 224.6ms\n",
      "Speed: 4.5ms preprocess, 224.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  92%|█████████▏| 46/50 [17:29<01:40, 25.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 724.4ms\n",
      "Speed: 13.0ms preprocess, 724.4ms inference, 30.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  94%|█████████▍| 47/50 [17:53<01:13, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 matricula, 160.4ms\n",
      "Speed: 11.2ms preprocess, 160.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  96%|█████████▌| 48/50 [18:13<00:46, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 matricula, 387.7ms\n",
      "Speed: 7.1ms preprocess, 387.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas:  98%|█████████▊| 49/50 [19:16<00:35, 35.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s] Matrícula: 3943FGF (Conf: 1.00)\n",
      "\n",
      "0: 480x640 1 matricula, 935.8ms\n",
      "Speed: 46.3ms preprocess, 935.8ms inference, 17.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesamiento de matrículas: 100%|██████████| 50/50 [19:41<00:00, 23.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARATIVA DE MODELOS ===\n",
      "Número de imágenes probadas: 50\n",
      "SmolVLM (aciertos): 0 / 50 (0.00%)\n",
      "EasyOCR (aciertos): 4 / 50 (8.00%)\n",
      "\n",
      "=== TIEMPO MEDIO DE INFERENCIA ===\n",
      "SmolVLM: 0.002 s/img\n",
      "EasyOCR: 23.150 s/img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH = \"matriculas_comparativa\"\n",
    "OUR_MODEL_PATH = 'best.pt'\n",
    "\n",
    "our_model = YOLO(OUR_MODEL_PATH)\n",
    "\n",
    "correct_vlm = 0\n",
    "correct_easy = 0\n",
    "time_vlm = 0.0\n",
    "time_easy = 0.0\n",
    "\n",
    "image_files = [\n",
    "    f for f in os.listdir(PATH)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\"))\n",
    "]\n",
    "\n",
    "\n",
    "for img_name in tqdm(image_files, desc=\"Procesamiento de matrículas\"):\n",
    "    img_path = os.path.join(PATH, img_name)\n",
    "    label = os.path.splitext(img_name)[0].upper()\n",
    "\n",
    "    frame = cv2.imread(img_path)\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    result = our_model(frame, show=False)\n",
    "    plate = result[0].boxes.xyxy.tolist()\n",
    "    x1, y1, x2, y2 = [int(item) for item in plate[0]]\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        text_vlm = ocr_vlm(plate, frame, x1, y1, x2, y2)\n",
    "    except Exception as e:\n",
    "        text_vlm = \"\"\n",
    "    time_vlm += time.time() - start\n",
    "    if text_vlm and text_vlm.strip().replace(\" \", \"\").upper() == label:\n",
    "        correct_vlm += 1\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        text_easy = ocr_easy(frame, frame, x1, y1)\n",
    "    except Exception as e:\n",
    "        print(f\"[EasyOCR error on {img_name}]: {e}\")\n",
    "        text_easy = \"\"\n",
    "    time_easy += time.time() - start\n",
    "    if text_easy and text_easy.strip().replace(\" \", \"\").upper() == label:\n",
    "        correct_easy += 1\n",
    "\n",
    "total = len(image_files)\n",
    "avg_time_vlm = time_vlm / total if total > 0 else 0\n",
    "avg_time_easy = time_easy / total if total > 0 else 0\n",
    "\n",
    "print(\"\\n=== COMPARATIVA DE MODELOS ===\")\n",
    "print(f\"Número de imágenes probadas: {total}\")\n",
    "print(f\"SmolVLM (aciertos): {correct_vlm} / {total} ({correct_vlm / total:.2%})\")\n",
    "print(f\"EasyOCR (aciertos): {correct_easy} / {total} ({correct_easy / total:.2%})\")\n",
    "\n",
    "print(\"\\n=== TIEMPO MEDIO DE INFERENCIA ===\")\n",
    "print(f\"SmolVLM: {avg_time_vlm:.3f} s/img\")\n",
    "print(f\"EasyOCR: {avg_time_easy:.3f} s/img\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
